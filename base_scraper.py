import logging
import sqlite3
from concurrent.futures import ThreadPoolExecutor
from queue import Queue, Empty
from threading import Thread

import requests

logging.basicConfig(
    filename="logs.log",
    filemode='a',
    datefmt='%H:%M:%S',
    level=logging.INFO
)
logging.getLogger().addHandler(logging.StreamHandler())


class Scraper:
    def __init__(
            self,
            file_name,
            num_workers,
            query_generator,
            query_scraper,
            dummy_data,
            cmt_interval=1000,
            timeout=10
    ):
        """
        Small scraper that uses multithreading. See test_scraper for an example.

        Parameters
        file_name:  sqlite file name\n
        num_workers:  no of concurrent scraper threads to run\n
        query_generator:  generator that yields queries\n
        query_scraper:  scrapes queries  generated by query_generator and returns a list of response dictionaries\n
        dummy_data: dummy data to initialize table (all fields are varchar)\n
        cmt_interval:  interval between commits\n
        timeout:  response queue monitor timeout\n
        """

        self.file_name = file_name
        self.num_workers = num_workers
        self.query_generator = query_generator
        self.query_scraper = query_scraper
        self.dummy_data = dummy_data
        self.cmt_interval = cmt_interval
        self.timeout = timeout
        self.current_count = 0
        self.response_queue = Queue()
        self.executor = None
        self.conn = None

    def init_db(self):
        self.conn = sqlite3.connect(self.file_name)
        sql_qry = f'''
            CREATE TABLE IF NOT EXISTS main ({', '.join(col_name + ' varchar' for col_name in self.dummy_data.keys())},
            UNIQUE  ({', '.join(list(self.dummy_data.keys()))}) ON CONFLICT  IGNORE );
            '''
        logging.info(sql_qry)
        self.conn.execute(sql_qry)
        self.conn.commit()

    def insert_data(self, data: dict):
        self.current_count = (self.current_count + 1) % self.cmt_interval
        cmt_stmt = (self.current_count == self.cmt_interval - 1)
        sql_qry = f'''INSERT INTO  main ({', '.join(data.keys())}) values ({',  '.join(['?'] * len(data.keys()))} );'''
        self.conn.execute(sql_qry, [str(v) for v in data.values()])
        if cmt_stmt:
            self.conn.commit()

    def response_queue_monitor(self):
        self.init_db()
        while True:
            try:
                data_dict = self.response_queue.get(timeout=self.timeout)
                self.insert_data(data_dict)
            except Empty:
                logging.info("All queries scraped")
                self.conn.commit()
                self.conn.close()
                break
            except:
                logging.error("Sqlite insert error", exc_info=True)

    def worker_func(self, query):
        try:
            for data_dict in self.query_scraper(query):
                self.response_queue.put(data_dict)
        except:
            logging.error(f"Error processing {query}, resubmitting to scrape")
            self.executor.submit(self.worker_func, query)

    def start_scraping(self):
        self.executor = ThreadPoolExecutor(self.num_workers)
        for query in self.query_generator():
            self.executor.submit(self.worker_func, query)

        response_consumer = Thread(target=self.response_queue_monitor)
        response_consumer.start()
        return response_consumer
